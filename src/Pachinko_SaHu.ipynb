{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc0941f",
   "metadata": {},
   "source": [
    "# Dynamic topic modelling with BERTopic\n",
    "### Coded by Sandrin Raphael Hunkeler (18-253-815)\n",
    "### Adapted from authors of the Script (C) Elisa Michelet, and Germans Savcisens (C).\n",
    "#### Url containing script: https://github.com/arobaselisa/industrial-west, https://github.com/carlomarxdk/topic_modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1a59e",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21743101",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-30T19:18:49.777879Z",
     "end_time": "2023-04-30T19:18:57.008229Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Siga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Siga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Siga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaleido==0.1.0post1 in c:\\users\\siga\\desktop\\textanalytics\\venv\\lib\\site-packages (0.1.0.post1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\siga\\desktop\\textanalytics\\venv\\lib\\site-packages (1.3.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tomotopy as tp\n",
    "import time\n",
    "import nltk\n",
    "import wordcloud as wordcloud\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "!pip3 install kaleido==0.1.0post1\n",
    "!pip3 install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Import Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84fceb3e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-30T19:18:49.824784Z",
     "end_time": "2023-04-30T19:19:00.478543Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siga\\AppData\\Local\\Temp\\ipykernel_14032\\4139990483.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  DATA = pd.read_csv(\n"
     ]
    },
    {
     "data": {
      "text/plain": "                file                                               text\nline                                                                   \n49996  200X.POS.rand                  I_PP beg_VBP to_TO move_VB ._SENT\n49997  200X.POS.rand  <p> Ms_NN Blears_NP :_: I_PP certainly_RB shal...\n49998  200X.POS.rand  <lb/> We_PP want_VBP to_TO make_VB improvement...\n49999  200X.POS.rand  <lb/> I_PP turn_VBP now_RB to_TO the_DT propos...\n50000  200X.POS.rand  <lb/> I_PP seek_VBP to_TO persuade_VB the_DT C...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>line</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>49996</th>\n      <td>200X.POS.rand</td>\n      <td>I_PP beg_VBP to_TO move_VB ._SENT</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>200X.POS.rand</td>\n      <td>&lt;p&gt; Ms_NN Blears_NP :_: I_PP certainly_RB shal...</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>200X.POS.rand</td>\n      <td>&lt;lb/&gt; We_PP want_VBP to_TO make_VB improvement...</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>200X.POS.rand</td>\n      <td>&lt;lb/&gt; I_PP turn_VBP now_RB to_TO the_DT propos...</td>\n    </tr>\n    <tr>\n      <th>50000</th>\n      <td>200X.POS.rand</td>\n      <td>&lt;lb/&gt; I_PP seek_VBP to_TO persuade_VB the_DT C...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siga\\AppData\\Local\\Temp\\ipykernel_14032\\4139990483.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  DATA = pd.read_csv(\n"
     ]
    },
    {
     "data": {
      "text/plain": "                file                                               text\nline                                                                   \n49996  200X.POS.rand                  I_PP beg_VBP to_TO move_VB ._SENT\n49997  200X.POS.rand  <p> Ms_NN Blears_NP :_: I_PP certainly_RB shal...\n49998  200X.POS.rand  <lb/> We_PP want_VBP to_TO make_VB improvement...\n49999  200X.POS.rand  <lb/> I_PP turn_VBP now_RB to_TO the_DT propos...\n50000  200X.POS.rand  <lb/> I_PP seek_VBP to_TO persuade_VB the_DT C...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file</th>\n      <th>text</th>\n    </tr>\n    <tr>\n      <th>line</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>49996</th>\n      <td>200X.POS.rand</td>\n      <td>I_PP beg_VBP to_TO move_VB ._SENT</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>200X.POS.rand</td>\n      <td>&lt;p&gt; Ms_NN Blears_NP :_: I_PP certainly_RB shal...</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>200X.POS.rand</td>\n      <td>&lt;lb/&gt; We_PP want_VBP to_TO make_VB improvement...</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>200X.POS.rand</td>\n      <td>&lt;lb/&gt; I_PP turn_VBP now_RB to_TO the_DT propos...</td>\n    </tr>\n    <tr>\n      <th>50000</th>\n      <td>200X.POS.rand</td>\n      <td>&lt;lb/&gt; I_PP seek_VBP to_TO persuade_VB the_DT C...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = pd.read_csv(\n",
    "    #\"../rawData/sampler_10ktexts_perdecade.headed.ALL2.tsv\",\n",
    "    \"../rawData/sampler_50ktexts_perdecade.headed.ALL.tsv\",\n",
    "    sep='\\t',\n",
    "    index_col=1\n",
    ")\n",
    "DATA.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "## 3. Preprocessing\n",
    "### 3.1 defining KEYWORDS and ROYALS name for selecting documents\n",
    "### 3.2 extend STOP_WORDS by highly frequent words found in general sentences"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T19:19:00.431552Z",
     "end_time": "2023-04-30T19:19:00.478543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "KEYWORDS = [\n",
    "    \"king\",\n",
    "    \"queen\",\n",
    "    \"royalty\",\n",
    "    \"majesty\",\n",
    "    \"crown\",\n",
    "    \"prince\",\n",
    "    \"princess\",\n",
    "    \"regent\",\n",
    "    \"monarch\",\n",
    "    \"highness\"\n",
    "]\n",
    "\n",
    "ROYALS = [\n",
    "    \"Queen Victoria\",\n",
    "    \"Queen Alexandra\",\n",
    "    \"Queen Mary\",\n",
    "    \"Queen Elizabeth\",\n",
    "    \"King George\",\n",
    "    \"King William\",\n",
    "    \"King Edward\",\n",
    "    \"King Edward\",\n",
    "    \"Prince Albert\",\n",
    "    \"Prince Edward\",\n",
    "    \"Prince George\",\n",
    "    \"Princess Victoria\",\n",
    "    \"Princess Louise\",\n",
    "    \"Princess Alexandra\",\n",
    "    \"Princess Mary\",\n",
    "    \"Princess Margaret\"\n",
    "]\n",
    "\n",
    "STOP_WORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "STOP_WORDS.update({\n",
    "    'hon',\n",
    "    'mr',\n",
    "    'gentleman',\n",
    "    'gent',\n",
    "    'gen',\n",
    "    'right',\n",
    "    'house',\n",
    "    'opinion',\n",
    "    'sir',\n",
    "    'say',\n",
    "    'friend',\n",
    "    'member',\n",
    "    'think',\n",
    "    'go',\n",
    "    'motion'\n",
    "    'noble',\n",
    "    'lord',\n",
    "    'case',\n",
    "    'year',\n",
    "    'speak',\n",
    "    'speech'\n",
    "})\n",
    "\n",
    "SPACY_LANGUAGE = \"en_core_web_sm\"\n",
    "COUNTRY = \"US\"\n",
    "LANG = \"en\"\n",
    "data = DATA"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T19:19:00.447240Z",
     "end_time": "2023-04-30T19:19:00.478543Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 remove functional words\n",
    "### 3.4 lemmatize words by POS tags"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def corpusRefactoring(data):\n",
    "    data.drop(data[~data['file'].astype(str).str.endswith('POS.rand')].index, inplace=True)\n",
    "    data['file'] = data['file'].map(lambda x: x.rstrip('X.POS.rand'))\n",
    "    data['file'] = data['file'].astype(str) + '0'\n",
    "    data['file'] = pd.to_datetime(data['file'], format='%Y')\n",
    "    data['file'] = pd.DatetimeIndex(data['file'])\n",
    "    data.rename(columns={\"file\":\"timePeriod\", \"text\":\"content\"}, inplace=True)\n",
    "    data.dropna(inplace = True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data\n",
    "\n",
    "def create_corpus(data):\n",
    "    raw_articles = data[\"content\"].to_list()\n",
    "    corpus = tp.utils.Corpus()\n",
    "    for doc in raw_articles:\n",
    "        if doc:\n",
    "            corpus.add_doc(doc.split())\n",
    "    return _,  corpus\n",
    "\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    if '_' not in word: return \"\"\n",
    "    token, posStart = word.split('_')[0].lower(), word.split('_')[1].upper()\n",
    "    if posStart.startswith('N'):\n",
    "        return wn.lemmatize(token, wordnet.NOUN)\n",
    "    elif posStart.startswith('V'):\n",
    "        return wn.lemmatize(token, wordnet.VERB)\n",
    "    elif posStart.startswith('R'):\n",
    "        return wn.lemmatize(token, wordnet.ADV)\n",
    "    elif posStart.startswith('J'):\n",
    "        return wn.lemmatize(token, wordnet.ADJ)\n",
    "    return \"\"\n",
    "\n",
    "def isRoyalTopic(text):\n",
    "    if len(text.split()) < 4: return False\n",
    "    overlapRoyals = any(word in ROYALS for word in text.split())\n",
    "    overlapKeyWords = any(word in KEYWORDS for word in text.split())\n",
    "    return overlapRoyals or overlapKeyWords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = simple_preprocess(text)\n",
    "    clean = [lemmatize(word) for word in tokens if lemmatize(word)!=\"\"]\n",
    "    words = [word for word in clean if word.isalpha()]\n",
    "    lemmas = [word for word in words if word not in STOP_WORDS]\n",
    "    return \" \".join(lemmas)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T19:19:00.478543Z",
     "end_time": "2023-04-30T19:19:00.498090Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a5c5b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-30T19:10:54.915230Z",
     "end_time": "2023-04-30T19:13:27.899283Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "corpusRefactoring(data)\n",
    "data.loc[:, 'content'] = data['content'].apply(preprocess_text).astype(str)\n",
    "data['royal'] = data['content'].apply(isRoyalTopic)\n",
    "data.drop(data[data.royal == False].index, inplace=True)\n",
    "data.dropna(inplace = True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#data = data.groupby(\"timePeriod\").sample(frac=0.01, random_state=42)\n",
    "processedData = data\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T19:14:37.844272Z",
     "end_time": "2023-04-30T19:14:37.875510Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 oversee the document frequencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.groupby(data[\"timePeriod\"].dt.year).size().plot(kind=\"bar\", figsize=(20, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T19:10:54.915230Z",
     "end_time": "2023-04-30T19:13:28.166890Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data.groupby(data[\"timePeriod\"].dt.year).size().plot(kind=\"bar\", figsize=(20, 3))\n",
    "plt.title(\"Number of Entries per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Entries\")\n",
    "plt.savefig(\"../diagramOutput/frequencyDistributionRawDocuments.png\", engine=\"kaleido\", scale=3, format='png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "f761bdf4",
   "metadata": {},
   "source": [
    "## 4. Pachinko Allocation Modell\n",
    "### 4.1 Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec839fd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-26T13:07:40.123298Z",
     "end_time": "2023-04-26T13:07:40.450134Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "_, corpus = create_corpus(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 4.2 Analyze optimal number of topics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f7800",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2023-04-26T13:07:40.450134Z",
     "end_time": "2023-04-26T14:35:51.442369Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "###### You should change only this\n",
    "k1_max = 3 ###(min 1) if k1_max - 1 == best k1\n",
    "k2_max = 25\n",
    "### k2 should always be higher than k1 (min 1)\n",
    "params = []\n",
    "scores = []\n",
    "num_iter = 0\n",
    "max_iter = 0\n",
    "for k1 in range(1, k1_max):\n",
    "    for k2 in range(k1,k2_max):\n",
    "        max_iter +=1\n",
    "        \n",
    "start = time.time()\n",
    "for k1 in range(1, k1_max):\n",
    "    for k2 in range(k1,k2_max):\n",
    "        num_iter+=1\n",
    "        model = tp.PAModel(tw=tp.TermWeight.IDF, min_df=10, rm_top=10, k1=k1, k2=k2, corpus=corpus, seed=0)\n",
    "        model.train(500, workers=1)\n",
    "        score = tp.coherence.Coherence(model, coherence=\"c_v\").get_score() #try with 'u_mass' instead of 'c_v' \n",
    "        params.append((k1,k2))\n",
    "        scores.append(score)\n",
    "        print(f\"Runtime: {int(time.time() - start)} seconds\", f\"|| Number of Searches: {num_iter}, {max_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8389c1",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T14:35:51.442369Z",
     "end_time": "2023-04-26T14:35:51.814854Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "best_params = np.argmax(scores)\n",
    "print(\"(BEST MODEL) k1=%s k2=%s coherence=%.2f\"  %(params[best_params][0], params[best_params][1],  scores[best_params]))\n",
    "\n",
    "data = pd.DataFrame(data={'k1':list(zip(*params))[0], 'k2':list(zip(*params))[1], 'score':scores})\n",
    "data = data.pivot(index='k1', columns='k2', values='score')\n",
    "sns.heatmap(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9cf03c",
   "metadata": {},
   "source": [
    "### 4.3 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_k1 = params[best_params][0]\n",
    "best_k2 = params[best_params][1]\n",
    "model = tp.PAModel(tw=tp.TermWeight.IDF, min_cf=5, k1=best_k1, k2=best_k2, corpus=corpus, seed=0)\n",
    "\n",
    "model.burn_in=100\n",
    "model.train(1000, workers=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T14:35:51.814854Z",
     "end_time": "2023-04-26T14:38:49.673218Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "df0e87cb",
   "metadata": {},
   "source": [
    "### 4.4 Print extracted topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b68a3",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T14:38:49.678775Z",
     "end_time": "2023-04-26T14:38:49.736602Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "for k in range(best_k2):\n",
    "    print(\"TOPIC\", k)\n",
    "    print(\"Words:\")\n",
    "    print([item[0] for item in model.get_topic_words(k, top_n=top_n)])\n",
    "    print(\"==========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 4.5 Visualize number of documents per topic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "infered, ll = model.infer(corpus)\n",
    "topic_data = np.zeros((len(infered),model.k2) )\n",
    "for i, doc in enumerate(infered):\n",
    "    temp = doc.get_sub_topics()\n",
    "    for item in temp:\n",
    "        topic_data[i, item[0]] = item[1]\n",
    "topic_data = pd.DataFrame(topic_data)\n",
    "topic_data[\"TOPIC\"] = topic_data.apply(lambda x: np.argmax(x), axis=1)\n",
    "topic_labels = topic_data[\"TOPIC\"].values\n",
    "plt.bar([\"Topic %s\" %i for i in range(model.k2)],topic_data[\"TOPIC\"].value_counts(), width=0.3)\n",
    "plt.title(\"Number of documents per topic\")\n",
    "plt.ylabel(\"# of documents\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## 5. Sentiments\n",
    "### 5.1 set up sentiment analyzer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "try:\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "except:\n",
    "    nltk.download('vader_lexicon')\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(doc):\n",
    "    \"\"\"\n",
    "    Helper function to estimate the sentiment scores\n",
    "    \"\"\"\n",
    "    return sia.polarity_scores(\" \".join([model.vocabs[w] for w in doc.words]))\n",
    "print(\"Analyzer is ready\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T14:39:02.584883Z",
     "end_time": "2023-04-26T14:39:02.872286Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 5.2 Process Sentiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentiment_data = np.zeros((len(infered), 4))\n",
    "for i, doc in enumerate(infered):\n",
    "    sentiment_data[i] = list(return_sentiment(doc).values())\n",
    "\n",
    "keys = list(return_sentiment(doc).keys())\n",
    "for i, k in enumerate(keys):\n",
    "    topic_data[k] = sentiment_data[:,i]\n",
    "print(\"Sentiments Calculated\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T14:39:02.856409Z",
     "end_time": "2023-04-26T14:39:18.984414Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### 5.3 Visualize Sentiments"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-30T18:32:36.765537Z",
     "end_time": "2023-04-30T18:32:36.781165Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import median_abs_deviation\n",
    "import wordcloud\n",
    "\n",
    "fig, ax = plt.subplots( model.k2 + 1,2, figsize=(20, 5 * (model.k2 + 1)))\n",
    "bins = np.linspace(-1,1, 20)\n",
    "################################\n",
    "########   Overall\n",
    "################################\n",
    "scores = topic_data[\"compound\"]\n",
    "ax[0,0].hist(scores, bins=bins)\n",
    "ax[0,0].set_title(\"Sentiment Distribution (Full), median: %.2f, mad: %.2f \" %(np.median(scores),\n",
    "                                                                                  median_abs_deviation(scores)))\n",
    "ax[0,0].set_ylabel(\"# of documents\")\n",
    "ax[0,0].set_xlabel(\"Sentiment Score\")\n",
    "ax[0,0].axvline(scores.median(), color='k', linestyle='dashed', linewidth=2)\n",
    "ax[0,0].set_xlim([-1,1])\n",
    "\n",
    "ax[0,1].imshow(np.zeros((10,10)))\n",
    "ax[0,1].set_title(\"Should be empty\")\n",
    "ax[0,1].axis(\"off\")\n",
    "###############################\n",
    "########    Per topic\n",
    "###############################\n",
    "wc = wordcloud.WordCloud(height=300, background_color=\"white\")\n",
    "for i in range(model.k2):\n",
    "    scores = topic_data[topic_data[\"TOPIC\"] == i][\"compound\"]\n",
    "    ax[i + 1,0].hist(scores, bins=bins)\n",
    "    ax[i + 1,0].set_ylabel(\"# of documents\")\n",
    "    ax[i + 1,0].set_xlim([-1,1])\n",
    "    ax[i + 1,0].set_xlabel(\"Sentiment Score\")\n",
    "    ax[i + 1,0].set_title(\"Sentiment Distribution (TOPIC %s), median: %.2f, mad: %.2f \" %(i, np.median(scores),\n",
    "                                                                                  median_abs_deviation(scores)))\n",
    "    ax[i + 1,0].axvline(scores.median(), color='k', linestyle='dashed', linewidth=2)\n",
    "\n",
    "    vocab = dict()\n",
    "    for item in model.get_topic_words(i, top_n=200):\n",
    "        vocab[item[0]] = item[1]\n",
    "    ax[i + 1,1].imshow(wc.generate_from_frequencies(vocab), interpolation=\"bilinear\")\n",
    "    ax[i + 1,1].set_title(\"WordCloud for Topic %s\" %i)\n",
    "    ax[i + 1,1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-26T15:04:30.071013Z",
     "end_time": "2023-04-26T15:04:54.162133Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
