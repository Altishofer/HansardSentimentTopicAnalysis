{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPA Model (Example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Dependencies and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T08:08:06.066990Z",
     "end_time": "2023-04-25T08:08:08.872003Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from typing import List\n",
    "import tomotopy as tp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from scipy.stats import median_abs_deviation\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.base import BaseEstimator\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from IPython.display import clear_output\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T08:08:08.872003Z",
     "end_time": "2023-04-25T08:08:09.466025Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load the SPACY English Library \n",
    "# (we desible some of the modules we do not use to increase speed)\n",
    "sp = spacy.load('en_core_web_sm', disable=[\"ner\",  \"entity_linker\",   \"parser\", \n",
    "                                           \"textcat\", \"textcat_multilabel\",  \"senter\",  \"sentencizer\",  \"transformer\"\n",
    "                                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T08:08:09.466025Z",
     "end_time": "2023-04-25T08:08:09.497512Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Helper functions\n",
    "def preprocess_docs(text: str, user_data = None) -> List[str]:\n",
    "    \"\"\" \n",
    "    Preprocess text: removes punctuation, digits, stop words, lemmatizes words\n",
    "    Args:\n",
    "        text: a string (e.g. sentence), not a list (lowercase is expected)\n",
    "        user_data: placeholder for the Tomotopy input (not used)\n",
    "    \"\"\"\n",
    "    lemmas = [word.lemma_ for word in sp(text) if word.is_alpha and (not word.is_stop)]  \n",
    "    return lemmas\n",
    "\n",
    "def chunker(iterable, total_length, chunksize):\n",
    "    \"\"\"\n",
    "    Chunk the iterable object for multithread processing\n",
    "    \"\"\"\n",
    "    return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize))\n",
    "\n",
    "def flatten(list_of_lists: List) -> List:\n",
    "    \"\"\"\n",
    "    Flatten a list of lists to a combined list\n",
    "    \"\"\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def preprocess_pipe(texts: list):\n",
    "    \"\"\"\n",
    "    Helper function for multithread processing of texts\n",
    "    \"\"\"\n",
    "    preproc_pipe = []\n",
    "    texts = [text.lower() for text in texts]\n",
    "    for doc in sp.pipe(texts, batch_size=48):\n",
    "        preproc_pipe.append(preprocess_docs(doc))\n",
    "    return preproc_pipe\n",
    "\n",
    "\n",
    "def preprocess_parallel(texts, num_docs: int, chunksize: int =100) -> List:\n",
    "    \"\"\"\n",
    "    Multithread processing of texts (e.g. sentences)\n",
    "    Args:\n",
    "        texts: list of strings (e.g. sentences)\n",
    "        num_docs: total number of strings in the list\n",
    "        chunksize: specifies how to split data across the threads\n",
    "\n",
    "    \"\"\"\n",
    "    executor = Parallel(n_jobs=7, backend='multiprocessing', prefer=\"processes\")\n",
    "    do = delayed(preprocess_pipe)\n",
    "    tasks = (do(chunk) for chunk in chunker(texts, num_docs, chunksize=chunksize))\n",
    "    result = executor(tasks)\n",
    "    return flatten(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T08:08:09.497512Z",
     "end_time": "2023-04-25T08:08:09.528526Z"
    }
   },
   "outputs": [],
   "source": [
    "class TopicWrapper(BaseEstimator):\n",
    "    \"\"\" \n",
    "    Wrapper for the Tomotopy HPA model. It simplifies the hyperparamaeter search with Sklearn.\n",
    "    \"\"\"\n",
    "    def __init__(self,  \n",
    "                k1: int, \n",
    "                k2: int, \n",
    "                top_n:int = 25, \n",
    "                train_iter: int = 500,\n",
    "                random_state: int = 0,\n",
    "                num_workers: int = 1,\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        self.random_state = random_state\n",
    "        self.k1 = k1\n",
    "        self.k2 = k2\n",
    "        self.train_iter = train_iter\n",
    "        self.top_n = top_n\n",
    "        self.num_workers = num_workers\n",
    "        self.model = None\n",
    "\n",
    "    def __init_model__(self):\n",
    "        \"\"\"Initialisez the HPA model with specific parameters\"\"\"\n",
    "        return tp.PAModel(tw=tp.TermWeight.PMI, min_cf=10, rm_top=1, \n",
    "                          k1=self.k1, k2=self.k2, seed=self.random_state)\n",
    "    def fit(self, X, **kwargs):\n",
    "        corpus = tp.utils.Corpus()\n",
    "        for doc in X:\n",
    "            if doc: \n",
    "                corpus.add_doc(doc)\n",
    "        self.model = self.__init_model__()\n",
    "        self.model.add_corpus(corpus)\n",
    "        self.model.burn_in = 100\n",
    "        self.model.train(self.train_iter, workers=self.num_workers)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        infered_corpus, ll = self.model.infer(X)\n",
    "        return infered_corpus, ll\n",
    "    def score(self, *args, **kwargs) -> float:\n",
    "        \"\"\"Returns the coherence score\"\"\"\n",
    "        return -tp.coherence.Coherence(self.model,coherence=\"u_mass\").get_score()\n",
    "    def set_params(self, **params):\n",
    "        self.model = None\n",
    "        return super().set_params(**params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data loading\n",
    "As an example, we load the Reuters dataset from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-25T08:08:09.512923Z",
     "end_time": "2023-04-25T08:08:09.591477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1516 entries, 0 to 1515\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   index       1516 non-null   int64 \n",
      " 1   timePeriod  1516 non-null   object\n",
      " 2   content     1516 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"../processedData/processedData.csv\",\n",
    "    sep=',',\n",
    "    index_col=0\n",
    ")\n",
    "data.dropna(inplace = True, how='any')\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.rename(columns={'processedText': 'content'}, inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "ids_ = [data[\"index\"].tolist()]\n",
    "raw_articles = [data[\"content\"].tolist()]\n",
    "NUM_DOCS = len(raw_articles)\n",
    "CHUNK_SIZE = 128\n",
    "processed = preprocess_parallel(raw_articles, NUM_DOCS, CHUNK_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = TopicWrapper(k1= 1, k2=1, top_n=50, num_workers=1, train_iter=750) # initialise simple model \n",
    "num_splits = 4 #cv_splits\n",
    "param_grid = list()\n",
    "\n",
    "# For HPA model: k1 <= k2\n",
    "# for other datasets you should probably increae the ranges \n",
    "for i in range(1,3):\n",
    "    for j in range(i, 6):\n",
    "        param_grid.append({\"k1\": [i], \"k2\": [j]})\n",
    "\n",
    "search = GridSearchCV(model, param_grid, cv=num_splits, n_jobs=12, verbose=2)\n",
    "result = search.fit(processed)\n",
    "clear_output()\n",
    "print(\"Best params (determined by CV):\")\n",
    "print(result.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get a more more reliable result so let's use a **One-Standard-Error rule** (i.e. cross-validation usually chooses over-complicated models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "se = np.array([std/np.sqrt(num_splits) for std in result.cv_results_[\"std_test_score\"]])\n",
    "means = result.cv_results_[\"mean_test_score\"]\n",
    "best_id = result.best_index_\n",
    "cutoff = means[best_id] - se[best_id]\n",
    "\n",
    "optimal_id = np.argwhere(means>cutoff)[0]\n",
    "optimal_params = param_grid[optimal_id.item()]\n",
    "print(\"Optimal Params:\")\n",
    "print(optimal_params)\n",
    "\n",
    "# A LITTLE BIT OF BOOKKEEPING (Optional)\n",
    "#with open(\"results/cv_res.pkl\", \"wb\") as f:\n",
    "#    pickle.dump({\"cv_res\": result.cv_results_,\n",
    "#                 \"best\": result.best_params_,\n",
    "#                 \"optimal\": optimal_params},f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training and Topic Overview\n",
    "Now that we know the optimal parameters, let's train our model on the full corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### Set the best model\n",
    "best_k1 = 1\n",
    "best_k2 = 4\n",
    "\n",
    "corpus = tp.utils.Corpus()\n",
    "for doc in processed:\n",
    "    if doc: \n",
    "        corpus.add_doc(doc)\n",
    "\n",
    "# we can (instead) use a TopicWrapper instead (but it is less straightforward)\n",
    "model = tp.PAModel(tw=tp.TermWeight.PMI, min_cf=10, rm_top=1, \n",
    "                   k1=best_k1,  k2= best_k2, corpus=corpus, seed=0)\n",
    "model.burn_in = 100\n",
    "model.train(1500, workers=1)\n",
    "\n",
    "model.save(\"model.obj\", full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "for k in range(best_k2):\n",
    "    print(\"TOPIC\", k)\n",
    "    print(\"Words:\")\n",
    "    print([item[0] for item in model.get_topic_words(k, top_n=top_n)])\n",
    "    print(\"==========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "infered, ll = model.infer(corpus)\n",
    "topic_data = np.zeros((len(infered),model.k2) )\n",
    "for i, doc in enumerate(infered):\n",
    "    temp = doc.get_sub_topics()\n",
    "    for item in temp:\n",
    "        topic_data[i, item[0]] = item[1]\n",
    "topic_data = pd.DataFrame(topic_data)\n",
    "topic_data[\"TOPIC\"] = topic_data.apply(lambda x: np.argmax(x), axis=1)\n",
    "topic_labels = topic_data[\"TOPIC\"].values\n",
    "plt.bar([\"Topic %s\" %i for i in range(model.k2)],topic_data[\"TOPIC\"].value_counts(), width=0.3)\n",
    "plt.title(\"Number of documents per topic\")\n",
    "plt.ylabel(\"# of documents\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Sentiment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load sentiment Analyser (based on VADER library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "try:   \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "except:\n",
    "    nltk.download('vader_lexicon')\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "def return_sentiment(doc):\n",
    "    \"\"\"\n",
    "    Helper function to estimate the sentiment scores\n",
    "    \"\"\"\n",
    "    return sia.polarity_scores(\" \".join([model.vocabs[w] for w in doc.words]))\n",
    "print(\"Analyzer is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sentiment_data = np.zeros((len(infered), 4))\n",
    "for i, doc in enumerate(infered):\n",
    "    sentiment_data[i] = list(return_sentiment(doc).values())\n",
    "\n",
    "keys = list(return_sentiment(doc).keys())\n",
    "for i, k in enumerate(keys):\n",
    "    topic_data[k] = sentiment_data[:,i]\n",
    "print(\"Sentiments Calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualise the scores and the word distribution per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( model.k2 + 1,2, figsize=(20, 5 * (model.k2 + 1)))\n",
    "bins = np.linspace(-1,1, 20)\n",
    "################################\n",
    "########   Overall\n",
    "################################\n",
    "scores = topic_data[\"compound\"]\n",
    "ax[0,0].hist(scores, bins=bins)\n",
    "ax[0,0].set_title(\"Sentiment Distribution (Full), median: %.2f, mad: %.2f \" %(np.median(scores),\n",
    "                                                                                  median_abs_deviation(scores)))\n",
    "ax[0,0].set_ylabel(\"# of documents\")\n",
    "ax[0,0].set_xlabel(\"Sentiment Score\")\n",
    "ax[0,0].axvline(scores.median(), color='k', linestyle='dashed', linewidth=2)\n",
    "ax[0,0].set_xlim([-1,1])\n",
    "\n",
    "ax[0,1].imshow(np.zeros((10,10)))\n",
    "ax[0,1].set_title(\"Should be empty\")\n",
    "ax[0,1].axis(\"off\")\n",
    "###############################\n",
    "########    Per topic \n",
    "###############################\n",
    "wc = wordcloud.WordCloud(height=300, background_color=\"white\")\n",
    "for i in range(model.k2):\n",
    "    scores = topic_data[topic_data[\"TOPIC\"] == i][\"compound\"]\n",
    "    ax[i + 1,0].hist(scores, bins=bins)\n",
    "    ax[i + 1,0].set_ylabel(\"# of documents\")\n",
    "    ax[i + 1,0].set_xlim([-1,1])\n",
    "    ax[i + 1,0].set_xlabel(\"Sentiment Score\")\n",
    "    ax[i + 1,0].set_title(\"Sentiment Distribution (TOPIC %s), median: %.2f, mad: %.2f \" %(i, np.median(scores),\n",
    "                                                                                  median_abs_deviation(scores)))\n",
    "    ax[i + 1,0].axvline(scores.median(), color='k', linestyle='dashed', linewidth=2)\n",
    "\n",
    "    vocab = dict()\n",
    "    for item in model.get_topic_words(i, top_n=200):\n",
    "        vocab[item[0]] = item[1]\n",
    "    ax[i + 1,1].imshow(wc.generate_from_frequencies(vocab), interpolation=\"bilinear\")\n",
    "    ax[i + 1,1].set_title(\"WordCloud for Topic %s\" %i)\n",
    "    ax[i + 1,1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40e38c8b90733f8388c40e54c1f99234f23e54be620e08c30d3adf4156981cec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
